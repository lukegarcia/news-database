# -*- coding: utf-8 -*-
"""news_ingestion

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ousxa3205rgNzRxFWfTkSPTk1iIyrDbV
"""

import time
import json
import re
import sqlite3
import hashlib
from datetime import datetime, timedelta, timezone
from urllib.parse import urlparse, urlunparse, parse_qsl, urlencode
import requests


DB_PATH = "wsj.db"

# API
# NEWSDATA_API_KEY = ""
ARCHIVE_URL = "https://newsdata.io/api/1/archive"

# Source Settings
DOMAIN = "wsj.com"
SOURCE_ID = "wsj.com"
SOURCE_NAME = "Wall Street Journal"

# Fetch Settings
DAYS_BACK = 90
MAX_ARTICLES = 5000
REQUEST_SLEEP = 0.2

# Initialize database
def init_db(db_path: str):
    conn = sqlite3.connect(db_path)
    cur = conn.cursor()

    # Sources Table
    cur.execute("""
    CREATE TABLE IF NOT EXISTS sources (
      source_id TEXT PRIMARY KEY,
      source_name TEXT NOT NULL
    );
    """)

    # Documents Table (tracks unique URLs)
    cur.execute("""
    CREATE TABLE IF NOT EXISTS documents (
      doc_id TEXT PRIMARY KEY,
      source_id TEXT NOT NULL,
      url TEXT NOT NULL,
      first_seen_at TEXT NOT NULL,
      FOREIGN KEY(source_id) REFERENCES sources(source_id)
    );
    """)

    # Snapshots table (Stores actual content versions)
    cur.execute("""
    CREATE TABLE IF NOT EXISTS snapshots (
      snapshot_id INTEGER PRIMARY KEY AUTOINCREMENT,
      doc_id TEXT NOT NULL,
      ingested_at TEXT NOT NULL,
      published_at TEXT,
      title TEXT,
      description TEXT,
      text TEXT NOT NULL,
      text_hash TEXT NOT NULL,
      raw_json TEXT,
      FOREIGN KEY(doc_id) REFERENCES documents(doc_id)
    );
    """)

    # Indexes
    cur.execute("CREATE INDEX IF NOT EXISTS idx_documents_url ON documents(url);")
    cur.execute("CREATE INDEX IF NOT EXISTS idx_snapshots_doc_id ON snapshots(doc_id);")
    cur.execute("CREATE INDEX IF NOT EXISTS idx_snapshots_published_at ON snapshots(published_at);")
    cur.execute("CREATE INDEX IF NOT EXISTS idx_snapshots_ingested_at ON snapshots(ingested_at);")
    cur.execute("CREATE INDEX IF NOT EXISTS idx_snapshots_text_hash ON snapshots(text_hash);")

    # Seed the source
    cur.execute("""
    INSERT OR IGNORE INTO sources (source_id, source_name)
    VALUES (?, ?);
    """, (SOURCE_ID, SOURCE_NAME))

    conn.commit()
    conn.close()

# Initialize DB immediately upon script run
init_db(DB_PATH)
print("DB initialized at:", DB_PATH)


# Helpers
def ensure_api_key():
    if not NEWSDATA_API_KEY.strip():
        raise ValueError("Set NEWSDATA_API_KEY at the top of the script.")

def normalize_url(url: str) -> str:
    """Strips tracking params to ensure unique URLs."""
    if not url:
        return ""
    url = url.strip().lower()
    parsed = urlparse(url)._replace(fragment="")
    tracking = {"utm_source","utm_medium","utm_campaign","utm_term","utm_content","fbclid","gclid"}
    params = parse_qsl(parsed.query, keep_blank_values=True)
    params = [(k,v) for k,v in params if k not in tracking]
    return urlunparse(parsed._replace(path=parsed.path.rstrip("/"), query=urlencode(params)))

def doc_id_from_url(url: str) -> str:
    return hashlib.sha256(normalize_url(url).encode("utf-8")).hexdigest()

def make_text(title, description) -> str:
    text = " ".join([title or "", description or ""])
    text = re.sub(r"\s+", " ", text.strip())
    return text

def text_hash(text: str) -> str:
    return hashlib.sha256(text.lower().encode("utf-8")).hexdigest()


# Data ingestion
def fetch_wsj_archive(days_back: int, max_articles: int):
    ensure_api_key()
    now = datetime.now(timezone.utc)
    from_date = (now - timedelta(days=days_back)).strftime("%Y-%m-%d")
    to_date = now.strftime("%Y-%m-%d")
    print(f"Fetching WSJ from {from_date} â†’ {to_date}")

    articles = []
    next_page = None

    while True:
        params = {
            "apikey": NEWSDATA_API_KEY,
            "domainurl": DOMAIN,
            "language": "en",
            "from_date": from_date,
            "to_date": to_date,
        }
        if next_page:
            params["page"] = next_page

        try:
            r = requests.get(ARCHIVE_URL, params=params, timeout=30)
            if r.status_code != 200:
                print("API error:", r.status_code, r.text[:300])
                break

            data = r.json()
            batch = data.get("results", []) or []
            articles.extend(batch)

            print(f"Fetched batch: {len(batch)} | Total so far: {len(articles)}")

            if len(articles) >= max_articles:
                break

            next_page = data.get("nextPage")
            if not next_page:
                break

            time.sleep(REQUEST_SLEEP)
        except Exception as e:
            print(f"Request failed: {e}")
            break

    return articles[:max_articles], now


def insert_articles(db_path: str, articles: list, ingested_at_iso: str):
    conn = sqlite3.connect(db_path)
    cur = conn.cursor()

    new_docs = 0
    new_snaps = 0
    skipped_empty = 0
    skipped_dupe = 0

    print(f"Starting insertion of {len(articles)} articles...")

    for a in articles:
        url = a.get("link") or a.get("url")
        if not url:
            continue

        doc_id = doc_id_from_url(url)
        text = make_text(a.get("title"), a.get("description"))
        if not text:
            skipped_empty += 1
            continue

        thash = text_hash(text)

        # Insert into Documents table if new URL
        cur.execute("SELECT 1 FROM documents WHERE doc_id=?", (doc_id,))
        if cur.fetchone() is None:
            cur.execute("""
                INSERT INTO documents (doc_id, source_id, url, first_seen_at)
                VALUES (?, ?, ?, ?)
            """, (doc_id, SOURCE_ID, url, ingested_at_iso))
            new_docs += 1

        # 2. Insert into Snapshots table if content is unique
        cur.execute("SELECT 1 FROM snapshots WHERE doc_id=? AND text_hash=? LIMIT 1", (doc_id, thash))
        if cur.fetchone() is not None:
            skipped_dupe += 1
            continue

        cur.execute("""
            INSERT INTO snapshots (
              doc_id, ingested_at, published_at,
              title, description, text, text_hash, raw_json
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            doc_id,
            ingested_at_iso,
            a.get("pubDate"),
            a.get("title"),
            a.get("description"),
            text,
            thash,
            json.dumps(a)
        ))
        new_snaps += 1

    conn.commit()
    conn.close()

    print("Ingestion done.")
    print(f"New documents: {new_docs}")
    print(f"New snapshots: {new_snaps}")
    print(f"Skipped empty text: {skipped_empty}")
    print(f"Skipped exact duplicates: {skipped_dupe}")


if __name__ == "__main__":
    try:
        articles, now = fetch_wsj_archive(DAYS_BACK, MAX_ARTICLES)
        if articles:
            insert_articles(DB_PATH, articles, now.isoformat())
        else:
            print("No articles fetched.")
    except Exception as e:
        print(f"An error occurred: {e}")